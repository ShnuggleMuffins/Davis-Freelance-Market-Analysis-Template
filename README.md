# Symphony-Market-Analysis-Template
A reproducible, **auditâ€‘ready** repository scaffold for AIâ€‘assisted marketâ€‘analysis projects.


---
## 1Â Â·Â Prerequisites

| Tool | Why you need it | Quick install |
|------|-----------------|---------------|
| **GitÂ >=Â 2.35** | versionâ€‘control & GitHub sync | macOS: `brew install git` â€¢ Windows: <https://git-scm.com/download/win> |
| **GitHub account** | remote repo + issues/wiki | <https://github.com> |
| **QuartoÂ CLIÂ >=Â 1.6** | singleâ€‘source PDFâ€¯+â€¯PPT build | <https://quarto.org/docs/get-started/> |
| **MinicondaÂ ( PythonÂ 3.11 )** | isolated env for notebooks & tests | <https://docs.conda.io/en/latest/miniconda.html> |
| **VSÂ CodeÂ (+Â QuartoÂ ext.)** *(optional)* | friendly editor | <https://code.visualstudio.com/> |

> **Tip**  
> If you prefer a GUI, install **GitHubâ€¯Desktop** instead of using the command line.

---
## 2Â Â·Â Oneâ€‘time setup

```bash
# 1.  Sign in on GitHub and create a **new public repo**
#     Name:  symphony-market-analysis-template

# 2.  Clone it locally (substitute your user):
$ git clone https://github.com/<USERNAME>/symphony-market-analysis-template.git
$ cd symphony-market-analysis-template

# 3.  Create & activate Python env
$ conda create -n symphony_analysis python=3.11 -y
$ conda activate symphony_analysis

# 4.  Install Quarto + deps (once)
$ pip install -r requirements.txt
```

Add your **Perplexity / Scite / Elicit** API keys to a local `.env` (never commit real keys):

```dotenv
# .env.example â†’ copy to .env and fill
PERPLEXITY_API_KEY=""  
SCITE_API_KEY=""  
ELICIT_API_KEY=""
```

---
## 3Â Â·Â Directory layout

```
â”œâ”€â”€ _quarto.yml          # project config (PDF + slides)
â”œâ”€â”€ README.md            # this file
â”œâ”€â”€ requirements.txt     # Python deps
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/             # wget or API pulls (immutable)
â”‚   â””â”€â”€ processed/       # cleaned CSV / Parquet
â”œâ”€â”€ registry/
â”‚   â””â”€â”€ registry.csv     # Source Registry (one row per citation)
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ notebook.qmd     # Quarto notebook â€“ main analysis
â”œâ”€â”€ reports/             # Autoâ€‘rendered outputs
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_links.py    # HTTP 200 checker
â”‚   â””â”€â”€ test_numbers.py  # numericâ€‘diff smoke test
â””â”€â”€ .gitignore
```

---
## 4Â Â·Â Key files

### `_quarto.yml`
```yaml
project:
  type: website      # enables PDF & slides in one build
  output-dir: reports

format:
  pdf:
    toc: true
    number-sections: true
  pptx: default

execute:
  freeze: auto       # caches code unless source changes
```

### `analysis/notebook.qmd` (stub)
```markdown
---
title: "Market Analysis â€“ TEMPLATE"
format: pdf
execute:
  echo: false
---

```{python}
#| label: setup
import pandas as pd
print("hello Symphony ðŸ§ ")
```

## 1Â Introduction

Textâ€¦
```

### `registry/registry.csv`
```csv
source_id,title,outlet,year,url,doi,access_date,content_sha256
```

### `tests/test_links.py`
```python
import csv, requests, pathlib
REG = pathlib.Path(__file__).parents[1] / "registry/registry.csv"
for row in csv.DictReader(REG.open()):
    r = requests.head(row["url"], allow_redirects=True, timeout=10)
    assert r.status_code < 400, f"{row['source_id']} broken â†’ {r.status_code}"
```

---
## 5Â Â·Â Typical workflow

1. **Scope** â€‘Â fill out `scoping_sheet.xlsx` (create your own) and commit.
2. **Source** â€‘Â run retrieval script â†’ writes snapshots into `data/raw/` + updates `registry.csv`.
3. **Synthesize** â€‘Â write / run code in `analysis/notebook.qmd`; `quarto render` produces PDF & PPTX.
4. **QA** â€‘Â `pytest` must pass; if it fails, fix links or numbers until green.
5. **Deliver** â€‘Â commit + push, then share `reports/Marketâ€‘Analysis.pdf` with client.

---
## 6Â Â·Â Zeroâ€‘toâ€‘firstâ€‘build cheatâ€‘sheet

```bash
# Render report
quarto render analysis/notebook.qmd

# Run QA
pytest

# Add & push
git add .
git commit -m "First skeleton build"
git push origin main
```

---
## 7Â Â·Â Extending the template

* **Automated builds** â€“ enable *GitHubÂ Actions* with a simple workflow that runs `conda install`, `quarto render`, and `pytest` on each push.
* **Vector search** â€“ add a `scripts/` folder with RAG pipeline using Llamaâ€‘Index.
* **Evidence pack** â€“ bundle `data/raw/` + `registry.csv` into a ZIP artifact during CI.

Happy analysing! ðŸš€
